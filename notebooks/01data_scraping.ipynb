{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping\n",
    "\n",
    "To circumvent the use of Google's BigQuery, in this notebook we use alternate means to load the gdelt database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all cells will overwrite current data. Do you want to continue? y/n\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Preventing the execution of the notebook to not overwrite data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mlower()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreventing the execution of the notebook to not overwrite data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Preventing the execution of the notebook to not overwrite data."
     ]
    }
   ],
   "source": [
    "print(\"\"\"Running all cells will overwrite current data. Do you want to continue? y/n\"\"\")\n",
    "answer = input()\n",
    "\n",
    "if answer.lower()[0] == 'n':\n",
    "    raise Exception(\"Preventing the execution of the notebook as to not overwrite data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt'\n",
    "\n",
    "# Once this cell is ran, it will take some time to finish (~1m) due to\n",
    "# the retrieval of large amounts of data which need to be written to file\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    with open('./data/01events.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(soup.prettify())\n",
    "else:\n",
    "    print(f'Failed to retrieve gdelt data. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest of this project will be the events between countries, so from the retrieved data at `events.txt` we select only **events**, and discard *mentions* and *global knowledge graphs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'gdeltv2\\/(.*?)\\.export\\.CSV\\.zip'\n",
    "last_date_time = None\n",
    "\n",
    "with (\n",
    "    open('./data/01events.txt', 'r') as in_file, \n",
    "    open('./data/02relevant_events.json', 'w') as out_file\n",
    "):\n",
    "    out_file.write('{\\n')\n",
    "\n",
    "    for line in in_file:\n",
    "        try:\n",
    "            line_url = line.strip().split()[2]\n",
    "            if not line_url.__contains__('export'):\n",
    "                continue  # We skip mentions and gkg's\n",
    "\n",
    "            match = re.search(pattern, line_url)\n",
    "\n",
    "            if not match:\n",
    "                print(\"Failed to match regex.\")\n",
    "                continue\n",
    "\n",
    "            date_time = match.group(1)\n",
    "\n",
    "            if not last_date_time:  # We write a temporary JSON file\n",
    "                out_file.write('\"' + date_time[:8] + '\":[\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "\n",
    "            elif date_time[:8] != last_date_time:\n",
    "                out_file.write('\\n],\\n')\n",
    "                out_file.write('\"' + date_time[:8] + '\":[\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "                \n",
    "            else:\n",
    "                out_file.write(',\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "\n",
    "            last_date_time = date_time[:8]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    out_file.write(']\\n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `relevant_events.json` file contains duplicate keys which isn't permisable. The code in the following cell fixes this by joining the values of such duplicate keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (\n",
    "    open('./data/02relevant_events.json', 'r') as in_file, \n",
    "    open('./data/03cleaned_events.json', 'w') as out_file\n",
    "):\n",
    "    data = json.load(in_file)\n",
    "    agg_data = {}\n",
    "\n",
    "    for key, values in data.items():\n",
    "        if not agg_data.get(key):\n",
    "            agg_data[key] = []\n",
    "        agg_data[key].extend(values)\n",
    "    \n",
    "    json.dump(agg_data, out_file, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
