{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEXVZ3lxFuVw"
   },
   "source": [
    "# Global confilcts prediction, and analysis of the relations between countries using GDELT database.\n",
    "\n",
    "*Introduction*: Hi!\n",
    "\n",
    "\n",
    "(Some parts of this notebook require user input to run cells. Please keep that in mind when running multiple cells at once.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy4R7byuO_GW"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yjh4TLAw_C3"
   },
   "source": [
    "Make sure to have the following libraries installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkSrQaYtN9EP",
    "outputId": "adc8a64b-453c-4e65-83f8-55e2c4eb3c50"
   },
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 numpy pandas jq tqdm fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CC-6adrrRdk2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import jq\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUmRPxo_xcr8"
   },
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYJ5zXb7Dbos"
   },
   "source": [
    "To circumvent the use of Google's BigQuery, in this notebook we use alternate means to load the gdelt database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imxCvVwbxpF6",
    "outputId": "b32f2969-4af1-4088-8d45-349a5aaa1847"
   },
   "outputs": [],
   "source": [
    "print(\"\"\"Running all cells will overwrite all current data. Do you want to continue? y/n\"\"\")\n",
    "answer = input()\n",
    "\n",
    "if answer.lower()[0] == 'n':\n",
    "    raise Exception(\"Preventing the execution of the notebook as to not overwrite data.\")\n",
    "\n",
    "if not os.path.exists(r'./data/'):\n",
    "    os.makedirs(r'./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6O4B9c5ixpDF"
   },
   "outputs": [],
   "source": [
    "url = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt'\n",
    "\n",
    "# Once this cell is ran, it will take some time to finish (~1m) due to\n",
    "# the retrieval and parsing of large amounts of data which need to be written to file\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    with open('./data/01events.txt', 'w') as file:\n",
    "        file.write(soup.prettify())\n",
    "else:\n",
    "    print(f'Failed to retrieve gdelt data. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqHKPQRCywaU"
   },
   "source": [
    "The main interest of this project will be the events between countries, so from the retrieved data at `01events.txt` we select only **events**, and discard *mentions* and *global knowledge graphs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WZJTmtZoxpAn"
   },
   "outputs": [],
   "source": [
    "pattern = r'http:\\/\\/data.gdeltproject.org\\/gdeltv2\\/(.*?)\\.export\\.CSV\\.zip'\n",
    "last_date_time = None\n",
    "\n",
    "with (\n",
    "    open('./data/01events.txt', 'r') as in_file,\n",
    "    open('./data/02relevant_events.json', 'w') as out_file\n",
    "):\n",
    "    out_file.write('{\\n')\n",
    "\n",
    "    for line in in_file:\n",
    "        try:\n",
    "            line_url = line.strip().split()[2]\n",
    "            if not line_url.__contains__('export'):\n",
    "                continue  # We skip mentions and gkg's\n",
    "\n",
    "            match = re.search(pattern, line_url)\n",
    "\n",
    "            if not match:\n",
    "                print(\"Failed to match regex.\")\n",
    "                continue\n",
    "\n",
    "            date_time = match.group(1)\n",
    "\n",
    "            if not last_date_time:  # We write a temporary JSON file\n",
    "                out_file.write('\"' + date_time[:8] + '\":[\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "\n",
    "            elif date_time[:8] != last_date_time:\n",
    "                out_file.write('\\n],\\n')\n",
    "                out_file.write('\"' + date_time[:8] + '\":[\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "\n",
    "            else:\n",
    "                out_file.write(',\\n')\n",
    "                out_file.write('{\"' + date_time[8:12] + '\":\"' + line_url + '\"}')\n",
    "\n",
    "            last_date_time = date_time[:8]\n",
    "        except:  # The line of data retrieved from the webpage is irregular and can be skipped\n",
    "            # print(f'Corrupt line in 01events.txt containing the following line: {line}')\n",
    "            continue\n",
    "\n",
    "    out_file.write(']\\n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niD0hpr21wLs"
   },
   "source": [
    "The `02relevant_events.json` file contains duplicate keys which isn't permisable. The code in the following cell fixes this by joining the values of such duplicate keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yK4troliwcIn"
   },
   "outputs": [],
   "source": [
    "def multidict(ordered_pairs):\n",
    "    data = {}\n",
    "\n",
    "    for key, value in ordered_pairs:\n",
    "        if len(key) == 4:\n",
    "            data[key] = value\n",
    "            continue\n",
    "\n",
    "        if not data.get(key):\n",
    "            data[key] = []\n",
    "        data[key].extend(value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with (\n",
    "    open('./data/02relevant_events.json', 'r') as in_file,\n",
    "    open('./data/03cleaned_events.json', 'w') as out_file\n",
    "):\n",
    "    data = json.load(in_file, object_pairs_hook=multidict)\n",
    "    json.dump(data, out_file, indent=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptn6aQXi18kX"
   },
   "source": [
    "After this step, the `03cleaned_events.json` file contains a structured way to retrieve all necessary links for the gdelt data indexed by date and then by time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7rmh4Dn2QtO"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bPv_KLxmxo7O"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(r'./data/03cleaned_events.json'):\n",
    "    raise Exception(\"There is no data to preprocess. Please run the cells in the Data Retrieval section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DCsa-H9GRndC"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jpqzGRAiRnkR"
   },
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    \"Global_Event_ID\", \"Day\", \"YYYYMM\", \"YYYY\", \"Day_Time\", \"Actor_1_Country_Code\", \"Actor_1_Name\",\n",
    "    \"Actor_1_Country_ABBR\", \"Actor_1_Known_Group_Code\", \"Actor_1_Ethnic_Code\", \"Actor_1_Religion_Code\",\n",
    "    \"Actor_1_Religion_2_Code\", \"Actor_1_Role\", \"Actor_1_Role2\", \"Actor_1_Role3\", \"Actor_2_Country_Code\",\n",
    "    \"Actor_2_Name\", \"Actor_2_Country_ABBR\", \"Actor_2_Know_Group_Code\", \"Actor_2_Ethnic_Code\",\n",
    "    \"Actor_2_Religion_Code\", \"Actor_2_Religion_2_Code\", \"Actor_2_Role\", \"Actor_2_Role2\", \"Actor_2_Role3\",\n",
    "    \"Is_Root_Event\", \"Event_Code\", \"Event_Base_Code\", \"Event_Root_Code\", \"Quad_Class\", \"Goldstein_Scale\",\n",
    "    \"Num_Mentions\", \"Num_Sources\", \"Num_Articles\", \"AVG_TONE\", \"Actor_1_Geo_Type\", \"Actor_1_Geo_FullName\",\n",
    "    \"Actor_1_Geo_Country_Code\", \"Actor1Geo_ADM1Code\", \"Actor1Geo_ADM2Code\", \"Actor1Geo_Lat\", \"Actor1Geo_Long\",\n",
    "    \"Actor1Geo_FeatureID\", \"Actor_2_Geo_Type\", \"Actor_2_Geo_FullName\", \"Actor_2_Geo_Country_Code\",\n",
    "    \"Actor2Geo_ADM1Code\", \"Actor2Geo_ADM2Code\", \"Actor2Geo_Lat\", \"Actor2Geo_Long\", \"Actor2Geo_FeatureID\",\n",
    "    \"Mention_Type\", \"ST_PR_CNTRY\", \"Country\", \"ADM1Code_Extra\", \"ADM2Code_Extra\", \"Lat_Extra\", \"Long_Extra\",\n",
    "    \"ActorGeo_FeaturID_Extra\", \"Date_Added\", \"Source_URL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V0vFEUyPW3vz"
   },
   "outputs": [],
   "source": [
    "mentions_col_names = [\n",
    "    'Global_Event_ID', 'Event_Time_Date', 'Mention_Time_Date', 'Mention_Type',\n",
    "    'Mention_Source_Name', 'Mention_Identifier', 'Sentence_ID',\n",
    "    'Actor_1_Char_Offset', 'Actor_2_Char_Offset', 'Action_Char_Offset',\n",
    "    'In_Raw_Text', 'Confidence', 'Mention_Doc_Len', 'Mention_Doc_Tone'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4VkqNbmhEVfe"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.gdeltproject.org/data/lookups/CAMEO.country.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    with open(r'./data/06country_codes.txt', 'w') as file:\n",
    "        file.write(soup.prettify())\n",
    "else:\n",
    "    print(f'Failed to retrieve gdelt country codes. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "zWwIlJdnEwbe",
    "outputId": "d840c8d5-31ae-483f-adb0-f6c64a1be852"
   },
   "outputs": [],
   "source": [
    "country_codes: dict[str, str] = {}\n",
    "\n",
    "with open(r'./data/06country_codes.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        code, *country = line.strip().split()\n",
    "        country = ' '.join(country)\n",
    "        country_codes[code] = country\n",
    "\n",
    "country_codes.pop('CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zcmjKiXiOq6a"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.gdeltproject.org/data/lookups/CAMEO.eventcodes.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    with open(r'./data/07event_codes.txt', 'w') as file:\n",
    "        file.write(soup.prettify())\n",
    "else:\n",
    "    print(f'Failed to retrieve gdelt event codes. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "uB54GDrpOqzG",
    "outputId": "e5bc894a-bf79-4be3-cb82-cbb319a484bb"
   },
   "outputs": [],
   "source": [
    "event_codes: dict[str, str] = {}\n",
    "\n",
    "with open(r'./data/07event_codes.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        code, *event_description = line.strip().split()\n",
    "        event_description = ' '.join(event_description)\n",
    "        event_codes[code] = event_description\n",
    "\n",
    "event_codes.pop('CAMEOEVENTCODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "s6PxYKVeIwpW"
   },
   "outputs": [],
   "source": [
    "def is_valid_country_code(code: str) -> bool:\n",
    "    codes = country_codes.keys()\n",
    "    return pd.notnull(code) and any([code in ccode for ccode in codes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ps-bOyK3HuS"
   },
   "source": [
    "We define auxilary functions to ease the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e7yh3QAA3QJV"
   },
   "outputs": [],
   "source": [
    "def format_time_yyyymmddhhmmss_to_str(\n",
    "        year: str | int, month: str | int, day: str | int,\n",
    "        hours: str | int, minutes: str | int = \"00\", seconds: str | int = \"00\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Transform given date time to string.\n",
    "    \"\"\"\n",
    "    times = list(map(str, [year, month, day, hours, minutes, seconds]))\n",
    "    for i in range(len(times)):\n",
    "        if len(times[i]) == 1:\n",
    "            times[i] = \"0\" + times[i]\n",
    "\n",
    "    return \"\".join(times)\n",
    "\n",
    "\n",
    "def format_time_yyyymmdd_to_str(\n",
    "        year: str | int, month: str | int, day: str | int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Transform given date to string.\n",
    "    \"\"\"\n",
    "    times = list(map(str, [year, month, day]))\n",
    "    for i in range(len(times)):\n",
    "        if len(times[i]) == 1:\n",
    "            times[i] = \"0\" + times[i]\n",
    "\n",
    "    return \"\".join(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6WnTHw3mvvjp"
   },
   "outputs": [],
   "source": [
    "def format_str_yyyymmdd_to_time_str(x: str) -> str:\n",
    "    return str(datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UnSwjfOvuLii"
   },
   "outputs": [],
   "source": [
    "def load_gdelt_from_url(url: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Load gdelt data from url on certain date time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Luk4 script kiddie hax time, bypassing rate limit 4tt3mpt\n",
    "    # < --------------------------------------------------------------- >\n",
    "    \n",
    "    # Create address list and randomly select one to add to header\n",
    "    ipv6_addresses = [\n",
    "        '1550:dc4:d62e:6c4:327d:51d3:be53:9e8f',\n",
    "        '402:d2b:b458:b8fa:b931:44b8:353e:234',\n",
    "        'd9fa:8e46:6d80:602:e98f:cd43:6d0c:af74',\n",
    "        '33e2:99aa:ab04:dc05:3fde:4ed5:d17c:b1c9',\n",
    "        'df34:2251:5433:98e5:9560:6916:33ba:947b',\n",
    "        '9b06:c07:c2b4:4de4:823b:49ff:b34:dcb6',\n",
    "        'cb2:f49:28ed:8eae:5719:aa7b:1b85:e903',\n",
    "        '6f9f:ff57:8d73:c18a:e74f:3b88:b964:114b',\n",
    "        '9180:ab16:6c99:5e6:b147:ed3f:fd7c:b6e7',\n",
    "        '7ae3:7e16:db65:99b1:bc55:a27b:29d4:cdb5',\n",
    "        '6e7b:481f:877c:10b4:d7ef:7d9d:1b9d:2032',\n",
    "        '218c:5222:7fcc:f286:896c:3a1b:2657:22c'\n",
    "    ]\n",
    "    random_ipv6_address = ipv6_addresses[random.randint(0, (len(ipv6_addresses)-1))]\n",
    "\n",
    "    # Sp00f us3r 4g3nt h34d3r f0r 3xtr4 hax \n",
    "\n",
    "    ua = UserAgent()\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent' : ua.random, \n",
    "        'X-Forwarded-For': '127.0.0.1' # Perhaps this will work better instead of random_ipv6\n",
    "    }\n",
    "    \n",
    "    # < --------------------------------------------------------------- >\n",
    "\n",
    "    \n",
    "    response = requests.get(url, stream=True, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(r'./data/04temp_15min_data.zip', 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    else:\n",
    "        print(f'Failed to retrieve gdelt data for url: {url}. Status code: {response.status_code}')\n",
    "        return None\n",
    "\n",
    "    with zipfile.ZipFile(r'./data/04temp_15min_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(r'./data/')\n",
    "\n",
    "    pattern = r'gdeltv2\\/(.*?)\\.export\\.CSV\\.zip'\n",
    "\n",
    "    match = re.search(pattern, url)\n",
    "\n",
    "    if not match:\n",
    "        print(f\"Failed to match regex. {url}\")\n",
    "        return None\n",
    "\n",
    "    date_time = match.group(1)\n",
    "\n",
    "    os.rename(r'./data/' + date_time + \".export.CSV\", r'./data/05temp_15min_data.CSV')\n",
    "\n",
    "    data = pd.read_csv(r'./data/05temp_15min_data.CSV', sep=r'\\t', engine='python', header=None, names=col_names, dtype={'Event_Code': str})\n",
    "\n",
    "    os.remove(r'./data/04temp_15min_data.zip')\n",
    "    os.remove(r'./data/05temp_15min_data.CSV')\n",
    "\n",
    "    # Drop the events where a country is absent\n",
    "    data = data.dropna(subset=['Actor_1_Country_ABBR', 'Actor_2_Country_ABBR'])\n",
    "    \n",
    "    # Drop the events where the country codes are equal\n",
    "    data = data[data['Actor_1_Country_ABBR'] != data['Actor_2_Country_ABBR']]\n",
    "    \n",
    "    # Filter only the countries/continents\n",
    "    data = data [\n",
    "                    data[['Actor_1_Country_ABBR', 'Actor_2_Country_ABBR']]\n",
    "                        .map(is_valid_country_code)\n",
    "                        .all(axis=1)\n",
    "                ]\n",
    "\n",
    "    data = (\n",
    "        # Get only the relevant columns of the data\n",
    "        data[\n",
    "                [\n",
    "                    'Global_Event_ID', 'Day', 'Actor_1_Country_ABBR',\n",
    "                    'Actor_2_Country_ABBR', 'Event_Code', 'Quad_Class',\n",
    "                    'Goldstein_Scale', 'Num_Articles', # 'AVG_TONE', 'Source_URL'\n",
    "                ]\n",
    "            ]\n",
    "        # Filter only the events that happened in the wanted time-frame\n",
    "            [\n",
    "                data['Day'] == int(date_time[:8])\n",
    "            ]\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    mentions_url = url.replace('export', 'mentions')\n",
    "\n",
    "    response = requests.get(mentions_url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(r'./data/04temp_15min_data.zip', 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    else:\n",
    "        print(f'Failed to retrieve gdelt data for url: {mentions_url}. Status code: {response.status_code}')\n",
    "        exit()\n",
    "\n",
    "    with zipfile.ZipFile(r'./data/04temp_15min_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(r'./data/')\n",
    "\n",
    "    os.rename(r'./data/' + date_time + \".mentions.CSV\", r'./data/05temp_15min_data.CSV')\n",
    "\n",
    "    mentions_data = pd.read_csv(r'./data/05temp_15min_data.CSV', sep=r'\\t', engine='python', header=None, names=mentions_col_names)\n",
    "\n",
    "    os.remove(r'./data/04temp_15min_data.zip')\n",
    "    os.remove(r'./data/05temp_15min_data.CSV')\n",
    "\n",
    "    mentions_data = (\n",
    "        # Get only the relevant columns of the data\n",
    "        mentions_data[['Global_Event_ID', 'Confidence']]\n",
    "        # For the same event, get the confidence score by averaging all scores | WE MIGHT WANT MAX!!\n",
    "            .groupby(by='Global_Event_ID', as_index=False)\n",
    "            .aggregate('mean')\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    mentions_data['Confidence'] = mentions_data['Confidence'].astype(int)\n",
    "\n",
    "    return pd.merge(data, mentions_data, how='left', on='Global_Event_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n3Gw4tzRRnmS"
   },
   "outputs": [],
   "source": [
    "def load_gdelt_by_yyyymmddhhmmss(\n",
    "        year: str | int, month: str | int, day: str | int,\n",
    "        hours: str | int, minutes: str | int, seconds: str | int = \"00\"\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Load the gdelt events dataset logged on given date time.\n",
    "    \"\"\"\n",
    "    date_time = format_time_yyyymmddhhmmss_to_str(year, month, day, hours, minutes)\n",
    "\n",
    "    url = (\n",
    "        r'http://data.gdeltproject.org/gdeltv2/' +\n",
    "        date_time +\n",
    "        r'.export.CSV.zip'\n",
    "    )\n",
    "\n",
    "    return load_gdelt_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k2sFOwwnRnsi"
   },
   "outputs": [],
   "source": [
    "def load_gdelt_by_yyyymmdd(\n",
    "        year: str | int, month: str | int, day: str | int\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Load the gdelt events dataset logged on given date.\n",
    "    Equivalent to getting all 96 15-minute interval datasets on given date.\n",
    "    \"\"\"\n",
    "    hours_list = list(range(24))\n",
    "    minutes_list = list(range(0, 60, 15))\n",
    "\n",
    "    data_frames = []\n",
    "    for hours in hours_list:\n",
    "        for minutes in minutes_list:\n",
    "            data = load_gdelt_by_yyyymmddhhmmss(year, month, day, hours, minutes)\n",
    "            if data is not None:\n",
    "                data_frames.append(data)\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5YiVAbz3rsVw"
   },
   "outputs": [],
   "source": [
    "def load_gdelt_from_to_yyyymmdd(\n",
    "        year_from: str|int, month_from: str|int, day_from: str|int,\n",
    "        year_to: str|int, month_to: str|int, day_to: str|int\n",
    ") -> pd.DataFrame|None:\n",
    "    \"\"\"\n",
    "    Load the gdelt events dataset between selected days (inclusive).\n",
    "    \"\"\"\n",
    "    start_date = format_time_yyyymmdd_to_str(year_from, month_from, day_from)\n",
    "    end_date = format_time_yyyymmdd_to_str(year_to, month_to, day_to)\n",
    "\n",
    "    with open(r'./data/03cleaned_events.json', 'r') as file:\n",
    "        json_data = json.dumps(json.load(file))\n",
    "\n",
    "    # JQ query to filter URLs between the given dates\n",
    "    jq_query = f'. | to_entries | map(select(.key >= \"{start_date}\" and .key <= \"{end_date}\")) | .[].value | .[] | .[]'\n",
    "\n",
    "    urls = jq.compile(jq_query).input(text=json_data).all()\n",
    "\n",
    "    data_frames = []\n",
    "    for url in urls:\n",
    "        data = load_gdelt_from_url(url)\n",
    "        if data is not None:\n",
    "            data_frames.append(data)\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True) if len(data_frames) > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB_6yO550BCb"
   },
   "source": [
    "### Posting to FASTAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Fgiayzk_0p_Q"
   },
   "outputs": [],
   "source": [
    "def custom_sigmoid(n: int) -> float:\n",
    "    return 1 / (1 + 1 / (np.e ** ((n - 50) / 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "yu3Sj6DrLQhc",
    "outputId": "56ea1f73-d700-4337-95dc-143ee0f3f7c2"
   },
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2019, 1, 2)\n",
    "end_date = datetime.datetime(2022, 1, 1)\n",
    "\n",
    "current_date = start_date\n",
    "error_date = None\n",
    "exponential_wait_time = 1\n",
    "\n",
    "total_days = (end_date - start_date).days + 1\n",
    "\n",
    "with tqdm(total=total_days) as pbar:\n",
    "    while current_date <= end_date:\n",
    "        times = [current_date.year, current_date.month, current_date.day]\n",
    "        try:\n",
    "            current_data = load_gdelt_by_yyyymmdd(*times)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            time.sleep(exponential_wait_time)\n",
    "    \n",
    "            if current_date == error_date:\n",
    "                exponential_wait_time *= 2\n",
    "            else:\n",
    "                error_date = current_date\n",
    "            continue\n",
    "    \n",
    "        exponential_wait_time = 1\n",
    "    \n",
    "        current_data['Country_Pairs'] = (\n",
    "            current_data.apply(lambda x: str(tuple(\n",
    "                sorted([x['Actor_1_Country_ABBR'], x['Actor_2_Country_ABBR']])\n",
    "            )), axis=1)\n",
    "        )\n",
    "    \n",
    "        current_data['Event_Score'] = (\n",
    "            current_data['Goldstein_Scale'] *\n",
    "            current_data['Confidence'].apply(lambda x: custom_sigmoid(x))\n",
    "        )\n",
    "    \n",
    "        current_data = current_data.groupby(['Country_Pairs', 'Day']).aggregate(\n",
    "            country_code_a=('Actor_1_Country_ABBR', 'first'),\n",
    "            country_code_b=('Actor_2_Country_ABBR', 'first'),\n",
    "            relations_score=('Event_Score', 'mean'),\n",
    "            num_verbal_coop=('Quad_Class', lambda x: (x == 1).sum()),\n",
    "            num_material_coop=('Quad_Class', lambda x: (x == 2).sum()),\n",
    "            num_verbal_conf=('Quad_Class', lambda x: (x == 3).sum()),\n",
    "            num_material_conf=('Quad_Class', lambda x: (x == 4).sum())\n",
    "           \n",
    "        ).reset_index()\n",
    "\n",
    "        current_data = current_data.drop(columns='Country_Pairs')\n",
    "        current_data = current_data.rename(columns={'Day': 'date'})\n",
    "        current_data['date'] = current_data['date'].apply(lambda x: format_str_yyyymmdd_to_time_str(str(x)))\n",
    "    \n",
    "\n",
    "        for row in current_data.index:\n",
    "            data_to_post = current_data.iloc[row].to_json()\n",
    "\n",
    "            try: \n",
    "                response = requests.post(url=r'http://127.0.0.1:8000/api/v1/relations/', data=data_to_post, headers={\n",
    "                    \"x-key\": os.getenv(\"API_KEY\")\n",
    "                })\n",
    "\n",
    "            except Exception as e: \n",
    "                print(response.content)\n",
    "                continue\n",
    "\n",
    "            try: \n",
    "                response = requests.post(url=r'https://gdelt-api-staging.filipovski.net/api/v1/relations/', data=data_to_post, headers={\n",
    "                    \"x-key\": os.getenv(\"API_KEY\")\n",
    "                })\n",
    "\n",
    "            except Exception as e: \n",
    "                print(response.content)\n",
    "                continue\n",
    "\n",
    "        \n",
    "        pbar.update(1)\n",
    "        current_date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url=r'http://127.0.0.1:8000/api/v1/relations/', data=data_to_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggv_09yprC5B"
   },
   "source": [
    "{\n",
    "   \"date\": \"2024-08-25\",\n",
    "   \"country_code_a\": \"string\",\n",
    "   \"country_code_b\": \"string\",\n",
    "   \"relations_score\": 0,\n",
    "   \"num_verbal_coop\": 0,\n",
    "   \"num_material_coop\": 0,\n",
    "   \"num_verbal_conf\": 0,\n",
    "   \"num_material_conf\": 0\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3lAWN3bZ21e"
   },
   "source": [
    "### Calculate Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJXZmdxH07Pc"
   },
   "source": [
    "GDELT-GRF conflict score should take into account:\n",
    "- The potential impact of the type of the events (goldstein scale, custom scale using event code, quad class, maybe avg tone)\n",
    "- The scale of the events (num articles, confidence)\n",
    "---\n",
    "Contents of master table:\n",
    "- `Pair of countries`: (AFG, AUS)\n",
    "- `Day`: 20220430\n",
    "- `Number of events with quad class 1`: 10\n",
    "- `Number of events with quad class 2`: 20\n",
    "- `Number of events with quad class 3`: 100\n",
    "- `Number of events with quad class 4`: 3\n",
    "- Custom aggregate measure of goldstein scale for all events:\n",
    " - `Goldstein scale`: -0.8\n",
    " - `Confidence of extracted event from article`: 100\n",
    " - ~`Average tone of article`: -1 $~~~~$ [is it necessary?]~\n",
    " - $avgs = \\sum_{\\text{events}} gs \\cdot \\sigma(conf)$\n",
    "\n",
    " where $\\sigma(n) = \\frac{e^Q}{1 + e^Q}$ and $Q = \\frac{n-50}{10}$ (shifted and scaled sigmoid)\n",
    "\n",
    " (if we want to add custom goldstein mappings $cgs$, we could replace the goldstein value with $$gs \\gets \\beta \\cdot gs + (1 - \\beta) \\cdot cgs$$ but even if this gives more predictive power, it will be less interpretable)\n",
    "---\n",
    "\n",
    "Daily score calculation which we will predict later:\n",
    "$$\\frac{\\sum_t \\alpha_t \\cdot avgs(t) \\cdot N_t}{\\sum_t N_t}$$\n",
    "where $N_t$ is the total number of events which is the sum of all events in the respective quad classes, i.e. $N_t = N_1 + N_2 + N_3 + N_4$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uy4R7byuO_GW"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
